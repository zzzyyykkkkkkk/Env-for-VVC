
# Deep Reinforcement Learning Environment for Voltage Control in Distribution Networks

A Pythonâ€¯toolkit built on OpenAIÂ Gym for **active voltage regulation** in the IEEEâ€‘123 distribution feeder.  
Agents learn to coordinate PV inverters, staticâ€¯var compensatorsâ€¯(SVCs) and battery storage so that all bus voltages stay within limits while network losses are minimised.

---

## âœ¨â€¯Key Features

| Category | Highlights |
|----------|------------|
| **Gymâ€‘compatible environments** | `PowerNetEnv_IEEE123` (standard) & `Safe_PowerNetEnv_IEEE123` (with convexâ€‘optimisation safetyâ€‘layer). |
| **Data management** | `GeneralPowerDataManager` loadsâ€¯/â€¯cleans multiâ€‘year load & PV CSVs, interpolates to 3â€‘min resolution and delivers training batches. |
| **Pandapower network builder** | `create_pandapower_net` constructs the IEEEâ€‘123 feeder, injects branch parameters and attaches PV, SVC and battery devices. |
| **Surrogate model pipeline** | `main.py` generates powerâ€‘flow samples, trains a PyTorch surrogate model, logs loss curves and error histograms. |
| **Training callbacks** | Custom Stableâ€‘Baselines3 callbacks stream episode reward, voltâ€‘dev, losses & SOC to CSV + TensorBoard. |

---

## ğŸ› ï¸â€¯Installation

```bash
# 1. clone
git clone https://github.com/<yourâ€‘username>/vvcâ€‘drlâ€‘env.git
cd vvcâ€‘drlâ€‘env

# 2. create & activate virtualenv (PythonÂ â‰¥â€¯3.8)
python -m venv venv
# Linux / macOS
source venv/bin/activate
# Windows
venv\Scripts\activate

# 3. install dependencies
pip install -r requirements.txt
````

### Core Python packages

* `gymnasium`
* `stable-baselines3`
* `pandapower`
* `torch`
* `numpy`, `pandas`, `cvxpy`
* `matplotlib`, `tqdm`, `scikit-learn`

### Prepare data

Place CSVs under:

```
data/
â”œâ”€ active_power-data/IEEE123/
â”œâ”€ reactive_power-data/IEEE123/
â””â”€ pv-data/IEEE123/
```

(The data download link isï¼š
 Modify paths in `env_config` if required.)

---

## ğŸ“â€¯Project Structure

```
.
â”œâ”€â”€ data.py                    # Data Storage
â”œâ”€â”€ data_manager.py            # timeâ€‘series loader / preâ€‘processor
â”œâ”€â”€ pandapower_net.py          # IEEEâ€‘123 builder
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ env422.py              # standard DRL Gym env
â”‚   â”œâ”€â”€ Safeenv512.py          # env with safety layer
â”‚   â””â”€â”€ BatteryUnit.py         # battery model
â”œâ”€â”€ train.py                   # PPO training example
â”œâ”€â”€ test.py                    # evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€â€¯QuickÂ Start

### 1. Test environment

```python
env_config = {
    "battery_list": [11, 33, 55, 80],
    "year": 2012,
    "month": 1,
    "day": 1,
    "train": True,
    "reward_fun_type": "Bowl",
    "network_info": {
        'pv_buses': [5, 11, 12, 25, 32, 35, 45, 51, 56, 66, 75, 82, 85, 101, 110],
        'pv_q_mvar': [0.10, 0.14, 0.12, 0.10, 0.14, 0.12, 0.10, 0.14, 0.12, 0.10, 0.10, 0.14, 0.12, 0.10, 0.14],
        'svc_buses': [31, 50, 74, 110],
        'q_mvar': [0.15, 0.15, 0.15, 0.15],
    },
    "activate_power_data_path": "../data/active_power-data/IEEE123/load_active.csv",
    "reactivate_power_data_path": "../data/reactive_power-data/IEEE123/load_reactive.csv",
    "pv_data_path": "../data/pv-data/IEEE123/pv_active.csv"
}

# åˆ›å»ºç¯å¢ƒ
env = Safe_PowerNetEnv_IEEE123(env_config)

# ç¯å¢ƒåˆå§‹åŒ–
state = env.reset()
print(f"Initial State: {state}")

# è¿›è¡Œå¤šä¸ªæ­¥éª¤çš„æµ‹è¯•
for step in range(480):  # æµ‹è¯•480ä¸ªæ—¶é—´æ­¥ä¹Ÿå°±æ˜¯ä¸€å¤©
    # éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
    action = np.random.uniform(low=-1, high=1, size=env.action_space.shape)

    # æ‰§è¡ŒåŠ¨ä½œ
    next_state, reward, done, _, info = env.step(action)

    # è¾“å‡ºä¿¡æ¯
    print(f"Step {step + 1}:")
    print(f"Reward: {reward}")
    print(f"Done: {done}")

    if done:
        print("Environment reset after reaching the end of the episode.")
        state = env.reset()  # é‡æ–°å¼€å§‹ä¸€ä¸ªæ–°çš„å›åˆ
    else:
        state = next_state  # æ›´æ–°å½“å‰çŠ¶æ€ä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€
```


---

## âš™ï¸â€¯Configuration

All environment parameters (battery buses, PV/SVC limits, data paths, reward design) live in the `env_config` dict at the top of each env file.
Adapt these to different feeders or hardware inverters.

---


---

## ğŸ“„â€¯License

Distributed under the **MIT License**.
See [`LICENSE`](LICENSE) for full text.

---

## ğŸ™â€¯Acknowledgements

* [Pandapower](https://www.pandapower.org/) for distributionâ€‘system modelling
* [Stableâ€‘Baselines3](https://github.com/DLR-RM/stable-baselines3) for DRL algorithms
* Inspired by [RL-ADN](https://github.com/ShengrenHou/RL-ADN) for reinforcement learning in active distribution networks
* Built upon concepts from [MAPDN](https://github.com/Future-Power-Networks/MAPDN) for multi-agent ADN control frameworks
* This work also uses a lot of AI help. For example, the README of this article is generated by AI.

Happy voltageâ€‘controlling! âš¡

```
```
